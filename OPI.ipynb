{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from watchdog.observers import Observer\n",
    "#from watchdog.events import FileSystemEventHandler\n",
    "\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "#import json\n",
    "#import time\n",
    "#import re\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "#import datetime\n",
    "#import time\n",
    "from itertools import product\n",
    "import generateDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDir = os.getcwd()\n",
    "tamales = os.path.join(rootDir,\"tamales_inc\")\n",
    "teinvento = os.path.join(rootDir,\"teinvento_inc\")\n",
    "rawDataPath = os.path.join(\"crudo\",\"generador\",\"fuente\")\n",
    "processedDataPath = os.path.join(\"procesado\",\"generador\",\"fuente\")\n",
    "dateDic = {\"Jan\":\"01\", \"Feb\":\"02\", \"Mar\":\"03\", \"Apr\":\"04\",\n",
    "           \"May\":\"05\", \"Jun\":\"06\", \"Jul\":\"07\", \"Aug\":\"08\",\n",
    "           \"Sep\":\"09\", \"Oct\":\"10\", \"Nov\":\"11\", \"Dec\":\"12\"}\n",
    "startDate = 0 #Esta variable puede ser útil para determinar fecha a partir de la cual quieres correr el código\n",
    "ventasCols = [\"year\", \"month\", \"country\", \"calorie_category\", \"flavor\", \"zone\", \"product_code\", \"product_name\", \"sales\"]\n",
    "factCols = [\"year\", \"month\", \"sales\", \"id_region\", \"id_product\"]\n",
    "prodCols = [\"id_product\", \"calorie_category\", \"product\", \"product_brand\", \"producer\"]\n",
    "regionCols = [\"id_region\", \"country\", \"region\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromDataPath(dataPath, cols):\n",
    "    dataFrames = []\n",
    "    for subdir, dirs, files in os.walk(dataPath):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                path2File = subdir+\"/\"+file\n",
    "                dataFrame = pd.read_csv(path2File, names = cols)\n",
    "                dataFrames.append(dataFrame)\n",
    "    df = pd.concat(dataFrames)    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMappingfile(tableName, realtimeFile = None, processedDataPath = processedDataPath):\n",
    "    \"\"\"Esta función devuelve region_table y product_table para los datos de tamales\"\"\"\n",
    "    processedLocation = os.path.join(rootDir,processedDataPath)\n",
    "    if not os.path.exists(processedLocation):\n",
    "        os.makedirs(processedLocation)\n",
    "    fileName = tableName+\"_tamales_inc.csv\"\n",
    "    MappingFile = os.path.join(processedLocation,fileName)\n",
    "    if os.path.isfile(MappingFile):\n",
    "        if realtimeFile:\n",
    "            mapping_df = pd.read_csv(path2TableFile)\n",
    "        else:\n",
    "            newtable_df = pd.read_csv(MappingFile)\n",
    "            print(\"This mapping file already exists for history data.\")\n",
    "            return(newtable_df)\n",
    "    else:\n",
    "        if tableName ==\"region_table\":\n",
    "            newCols = [\"country\", \"zone\"]\n",
    "            #We need a new index for region_id\n",
    "            keepIndex = True\n",
    "        elif tableName ==\"product_table\":\n",
    "            newCols = [\"product_code\",\"product_name\",\"flavor\",\"calorie_category\"]\n",
    "            keepIndex = False\n",
    "        else:\n",
    "            print(\"Only region_table and product_table are supported as mapping file.\")\n",
    "            return\n",
    "        dataFrames = []\n",
    "        for subdir, dirs, files in os.walk(tamales):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    path2File = subdir+\"/\"+file\n",
    "                    dataFrame = pd.read_csv(path2File, names = ventasCols)\n",
    "                    dataFrames.append(dataFrame)\n",
    "        df = pd.concat(dataFrames)\n",
    "        \n",
    "        newtable_df = df[newCols].drop_duplicates().reset_index(drop=True) \n",
    "        if keepIndex:\n",
    "            newtable_df.index.name = \"id_region\"\n",
    "        #product_code has a one to many mapping issue which will be overwritten\n",
    "        newtable_df.to_csv(MappingFile,index=keepIndex,sep=\",\")\n",
    "    return(newtable_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createData(newDataPath, df, clearData, datos, macro=False):\n",
    "    \"\"\"Toma DataFrame (df) y lo carga en el nuevo path (newDataPath) utilizando una etiqueta como macro. \n",
    "    Limpiará el cache si clearData es True\"\"\"\n",
    "    if macro:\n",
    "        filename = datos+\"_\"+macro+\".csv\"\n",
    "    else:\n",
    "        filename = datos+\".csv\"\n",
    "    newFile = os.path.join(newDataPath,filename)\n",
    "    #If file exists and we don't need to clear data\n",
    "    if clearData:\n",
    "            if not os.path.exists(newDataPath):\n",
    "                os.makedirs(newDataPath)\n",
    "            print(\"Limpiando y cargando datos para \"+filename)\n",
    "            df.to_csv(newFile,index=False,sep=\",\")\n",
    "    else:\n",
    "        if os.path.isfile(newFile) :\n",
    "            print(\"Datos ya existen para \"+filename)\n",
    "            return\n",
    "        else:\n",
    "            if not os.path.exists(newDataPath):\n",
    "                os.makedirs(newDataPath)\n",
    "            print(\"Cargando datos para \"+filename)\n",
    "            df.to_csv(newFile,index=False,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load2PathYYYYMM(df, destPath, yearCol = \"year\", monthCol = \"month\", datos=\"tamales_inc\", clearData = False, startDate = startDate):\n",
    "    \"\"\"\n",
    "    Carga datos que se encuentran en el mismo directorio y los acomoda en datos crudos de acuerdo a su fecha.\n",
    "    \"\"\"\n",
    "    #We get 6 digit pattern\n",
    "    dfGrouped = df[[yearCol,monthCol]].groupby([yearCol,monthCol]).size().reset_index().drop(columns=[0])\n",
    "    \n",
    "    for index, rows in dfGrouped.iterrows():\n",
    "        year = rows[yearCol]\n",
    "        month = rows[monthCol]\n",
    "        YYYYMM = str(year)+dateDic[month]\n",
    "        #Aquí podemos cargar datos desde la fecha en que especifiquemos\n",
    "        if startDate > int(YYYYMM):\n",
    "            continue\n",
    "        newDestPath = os.path.join(rootDir,destPath,YYYYMM)\n",
    "        #We filter records having year and month values\n",
    "        years = df[yearCol] == year\n",
    "        months = df[monthCol] == month\n",
    "        dataWithDate = df.loc[years & months]\n",
    "        \n",
    "        createData(newDestPath, dataWithDate, clearData, datos, YYYYMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTamalRaw2Proc(rawDataPath = rawDataPath, processedDataPath=processedDataPath, clearData = False, startDate = startDate):\n",
    "    \"\"\"\n",
    "    Loads processed data from Raw data. \n",
    "    \"\"\"\n",
    "    region_df = createMappingfile(\"region_table\").reset_index()\n",
    "    product_df = createMappingfile(\"product_table\")\n",
    "    datos = \"tamales_inc\"\n",
    "    #We use regex to make sure we only load tamales\n",
    "    YYYYMMRegex = re.compile('({})_(\\d\\d\\d\\d\\d\\d)'.format(datos))\n",
    "    path = os.path.join(rootDir,rawDataPath)\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if not YYYYMMRegex.match(file):\n",
    "                continue\n",
    "            match = YYYYMMRegex.search(file)\n",
    "            YYYYMM = match.group(2) \n",
    "            #Aquí podemos cargar datos desde la fecha en que especifiquemos\n",
    "            if startDate > int(YYYYMM):\n",
    "                continue\n",
    "            newProcessedDataPath = os.path.join(rootDir,processedDataPath,YYYYMM)\n",
    "            #We keep relevant data\n",
    "            df1 = pd.read_csv(os.path.join(subdir,file))[[\"year\", \"month\", \"product_code\", \"zone\", \"sales\"]]\n",
    "            #We map region_id to zone\n",
    "            df1['zone'] = df1['zone'].map(region_df.set_index('zone')['id_region'])\n",
    "            df1Grouped = df1[[\"product_code\",\"zone\",\"sales\"]].groupby([\"product_code\",\"zone\"]).sum().reset_index()\n",
    "            df1Grouped.rename(columns = {\"product_code\":\"product\",\"sales\":\"monthly_sales\", \"zone\":\"id_region\"}, inplace = True)\n",
    "            \n",
    "            #Pendiente: arreglar error en 202003 cuando se incluyen nuevos valores\n",
    "            if 202002 < int(YYYYMM):\n",
    "                break\n",
    "            #Empezando el año\n",
    "            if file.endswith(\"01.csv\"):\n",
    "                df1Grouped[\"monthly_sales_acc\"] = df1Grouped[\"monthly_sales\"]\n",
    "                df1Grouped[\"diff_prev_month_perc\"] = None\n",
    "                df1Grouped_prev = df1Grouped.copy()\n",
    "            else:\n",
    "                #Aseguramos que el se creen columnas con datos para producto nuevo\n",
    "                df1Grouped[\"monthly_sales_acc\"] = df1Grouped[\"monthly_sales\"]\n",
    "                df1Grouped[\"diff_prev_month_perc\"] = None\n",
    "                for index, rows in df1Grouped_prev.iterrows():\n",
    "                        \n",
    "                    \n",
    "                    #Calculamos nuevo acumulado\n",
    "                    df1Grouped.loc[(df1Grouped[\"product\"]==rows[\"product\"]) & (df1Grouped[\"id_region\"]==rows[\"id_region\"]),\n",
    "                                   [\"monthly_sales_acc\"]] += rows[\"monthly_sales_acc\"]\n",
    "                    #Calculamos nuevo porcentaje\n",
    "                    prevSales = df1Grouped.loc[(df1Grouped[\"product\"]==rows[\"product\"]) & (df1Grouped[\"id_region\"]==rows[\"id_region\"]),\n",
    "                                               \"monthly_sales\"].item()\n",
    "                    newPercentage = ((prevSales/rows[\"monthly_sales\"] - 1)*100)\n",
    "                    df1Grouped.loc[(df1Grouped[\"product\"]==rows[\"product\"]) & (df1Grouped[\"id_region\"]==rows[\"id_region\"]),\n",
    "                               [\"diff_prev_month_perc\"]] = newPercentage\n",
    "\n",
    "                df1Grouped_prev = df1Grouped.copy()\n",
    "            \n",
    "            createData(newProcessedDataPath, df1Grouped, clearData, datos, YYYYMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def placeRaw2path(folder, destPath, rootDir=rootDir, clearData = False):\n",
    "    path2Folder = os.path.join(rootDir,folder)\n",
    "    newDataPath = os.path.join(rootDir,destPath)\n",
    "    for table in os.listdir(path2Folder):\n",
    "        subDir = os.path.join(path2Folder,table)\n",
    "        if table == \"fact_table\":\n",
    "            #Acomodaremos estos datos por fecha uniendo todas las particiones\n",
    "            dfData = loadFromDataPath(subDir,factCols)\n",
    "            load2PathYYYYMM(dfData,destPath,datos=folder)\n",
    "        elif table == \"product_dim\":\n",
    "            #Acomodaremos estos datos en el directorio fuente uniendo todas las particiones\n",
    "            dfData = loadFromDataPath(subDir,prodCols)\n",
    "            dfData = dfData[[\"id_product\", \"calorie_category\", \"product\", \"producer\"]]\n",
    "            createData(newDataPath, dfData, clearData, \"product_dim\")\n",
    "        else:\n",
    "            #Acomodaremos estos datos en el directorio fuente uniendo todas las particiones\n",
    "            dfData = loadFromDataPath(subDir,regionCols)\n",
    "            createData(newDataPath, dfData, clearData, \"region_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTamalesInc():    \n",
    "    print(\"Cargando datos de Tamales Inc.\")\n",
    "    df = loadFromDataPath(tamales, ventasCols)\n",
    "    print(\"Cargando datos crudos...\")\n",
    "    load2PathYYYYMM(df, rawDataPath)\n",
    "    print(\"Cargando datos procesados...\")\n",
    "    loadTamalRaw2Proc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTeinventoInc():\n",
    "    \"\"\"No proceso estos datos, sólo los ubico en los dataPath mencionados, y los separo si tienen columna year y month\"\"\"\n",
    "    print(\"Cargando datos de Teinvento Inc.\")\n",
    "    print(\"Cargando datos crudos...\")\n",
    "    placeRaw2path(\"teinvento_inc\", rawDataPath)\n",
    "    print(\"Cargando datos procesados...\")\n",
    "    placeRaw2path(\"teinvento_inc\", processedDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertRowsTamales(con_tamales):\n",
    "    generateDB.sqlInsertRegion(con_tamales, os.path.join(rootDir,processedDataPath,\"region_table_tamales_inc.csv\"))\n",
    "    generateDB.sqlInsertProductTamales(con_tamales, os.path.join(rootDir,processedDataPath,'product_table_tamales_inc.csv'))\n",
    "    datos = \"tamales_inc\"\n",
    "    YYYYMMRegex = re.compile('({})_(\\d\\d\\d\\d\\d\\d)'.format(datos))\n",
    "    path = os.path.join(rootDir,rawDataPath)\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if YYYYMMRegex.match(file):\n",
    "                match = YYYYMMRegex.search(file)\n",
    "                YYYYMM = match.group(2) \n",
    "                newFile = os.path.join(rootDir,processedDataPath,YYYYMM,file)\n",
    "                generateDB.sqlInsertTamalesInc(con_tamales, newFile, YYYYMM)\n",
    "def insertRowsTeinvento(con_teinvento):\n",
    "    generateDB.sqlInsertRegion(con_teinvento, os.path.join(rootDir,processedDataPath,\"region_dim.csv\"))\n",
    "    generateDB.sqlInsertProductTeinvento(con_teinvento, os.path.join(rootDir,processedDataPath,'product_dim.csv'))\n",
    "    datos = \"teinvento_inc\"\n",
    "    YYYYMMRegex = re.compile('({})_(\\d\\d\\d\\d\\d\\d)'.format(datos))\n",
    "    path = os.path.join(rootDir,rawDataPath)\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if YYYYMMRegex.match(file):\n",
    "                match = YYYYMMRegex.search(file)\n",
    "                YYYYMM = match.group(2) \n",
    "                newFile = os.path.join(rootDir,processedDataPath,YYYYMM,file)\n",
    "                generateDB.sqlInsertTeinventoInc(con_teinvento, newFile, YYYYMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de Tamales Inc.\n",
      "Cargando datos crudos...\n",
      "Cargando datos para tamales_inc_201904.csv\n",
      "Cargando datos para tamales_inc_201908.csv\n",
      "Cargando datos para tamales_inc_201912.csv\n",
      "Cargando datos para tamales_inc_201902.csv\n",
      "Cargando datos para tamales_inc_201901.csv\n",
      "Cargando datos para tamales_inc_201907.csv\n",
      "Cargando datos para tamales_inc_201906.csv\n",
      "Cargando datos para tamales_inc_201903.csv\n",
      "Cargando datos para tamales_inc_201905.csv\n",
      "Cargando datos para tamales_inc_201911.csv\n",
      "Cargando datos para tamales_inc_201910.csv\n",
      "Cargando datos para tamales_inc_201909.csv\n",
      "Cargando datos para tamales_inc_202004.csv\n",
      "Cargando datos para tamales_inc_202008.csv\n",
      "Cargando datos para tamales_inc_202002.csv\n",
      "Cargando datos para tamales_inc_202001.csv\n",
      "Cargando datos para tamales_inc_202007.csv\n",
      "Cargando datos para tamales_inc_202006.csv\n",
      "Cargando datos para tamales_inc_202003.csv\n",
      "Cargando datos para tamales_inc_202005.csv\n",
      "Cargando datos procesados...\n",
      "Cargando datos para tamales_inc_201901.csv\n",
      "Cargando datos para tamales_inc_201902.csv\n",
      "Cargando datos para tamales_inc_201903.csv\n",
      "Cargando datos para tamales_inc_201904.csv\n",
      "Cargando datos para tamales_inc_201905.csv\n",
      "Cargando datos para tamales_inc_201906.csv\n",
      "Cargando datos para tamales_inc_201907.csv\n",
      "Cargando datos para tamales_inc_201908.csv\n",
      "Cargando datos para tamales_inc_201909.csv\n",
      "Cargando datos para tamales_inc_201910.csv\n",
      "Cargando datos para tamales_inc_201911.csv\n",
      "Cargando datos para tamales_inc_201912.csv\n",
      "Cargando datos para tamales_inc_202001.csv\n",
      "Cargando datos para tamales_inc_202002.csv\n",
      "Cargando datos de Teinvento Inc.\n",
      "Cargando datos crudos...\n",
      "Cargando datos para teinvento_inc_201804.csv\n",
      "Cargando datos para teinvento_inc_201808.csv\n",
      "Cargando datos para teinvento_inc_201812.csv\n",
      "Cargando datos para teinvento_inc_201802.csv\n",
      "Cargando datos para teinvento_inc_201801.csv\n",
      "Cargando datos para teinvento_inc_201807.csv\n",
      "Cargando datos para teinvento_inc_201806.csv\n",
      "Cargando datos para teinvento_inc_201803.csv\n",
      "Cargando datos para teinvento_inc_201805.csv\n",
      "Cargando datos para teinvento_inc_201811.csv\n",
      "Cargando datos para teinvento_inc_201810.csv\n",
      "Cargando datos para teinvento_inc_201809.csv\n",
      "Cargando datos para teinvento_inc_201904.csv\n",
      "Cargando datos para teinvento_inc_201908.csv\n",
      "Cargando datos para teinvento_inc_201912.csv\n",
      "Cargando datos para teinvento_inc_201902.csv\n",
      "Cargando datos para teinvento_inc_201901.csv\n",
      "Cargando datos para teinvento_inc_201907.csv\n",
      "Cargando datos para teinvento_inc_201906.csv\n",
      "Cargando datos para teinvento_inc_201903.csv\n",
      "Cargando datos para teinvento_inc_201905.csv\n",
      "Cargando datos para teinvento_inc_201911.csv\n",
      "Cargando datos para teinvento_inc_201910.csv\n",
      "Cargando datos para teinvento_inc_201909.csv\n",
      "Cargando datos para teinvento_inc_202004.csv\n",
      "Cargando datos para teinvento_inc_202002.csv\n",
      "Cargando datos para teinvento_inc_202001.csv\n",
      "Cargando datos para teinvento_inc_202003.csv\n",
      "Cargando datos para teinvento_inc_202005.csv\n",
      "Cargando datos para product_dim.csv\n",
      "Cargando datos para region_dim.csv\n",
      "Datos ya existen para region_dim.csv\n",
      "Cargando datos procesados...\n",
      "Cargando datos para teinvento_inc_201804.csv\n",
      "Cargando datos para teinvento_inc_201808.csv\n",
      "Cargando datos para teinvento_inc_201812.csv\n",
      "Cargando datos para teinvento_inc_201802.csv\n",
      "Cargando datos para teinvento_inc_201801.csv\n",
      "Cargando datos para teinvento_inc_201807.csv\n",
      "Cargando datos para teinvento_inc_201806.csv\n",
      "Cargando datos para teinvento_inc_201803.csv\n",
      "Cargando datos para teinvento_inc_201805.csv\n",
      "Cargando datos para teinvento_inc_201811.csv\n",
      "Cargando datos para teinvento_inc_201810.csv\n",
      "Cargando datos para teinvento_inc_201809.csv\n",
      "Cargando datos para teinvento_inc_201904.csv\n",
      "Cargando datos para teinvento_inc_201908.csv\n",
      "Cargando datos para teinvento_inc_201912.csv\n",
      "Cargando datos para teinvento_inc_201902.csv\n",
      "Cargando datos para teinvento_inc_201901.csv\n",
      "Cargando datos para teinvento_inc_201907.csv\n",
      "Cargando datos para teinvento_inc_201906.csv\n",
      "Cargando datos para teinvento_inc_201903.csv\n",
      "Cargando datos para teinvento_inc_201905.csv\n",
      "Cargando datos para teinvento_inc_201911.csv\n",
      "Cargando datos para teinvento_inc_201910.csv\n",
      "Cargando datos para teinvento_inc_201909.csv\n",
      "Cargando datos para teinvento_inc_202004.csv\n",
      "Cargando datos para teinvento_inc_202002.csv\n",
      "Cargando datos para teinvento_inc_202001.csv\n",
      "Cargando datos para teinvento_inc_202003.csv\n",
      "Cargando datos para teinvento_inc_202005.csv\n",
      "Cargando datos para product_dim.csv\n",
      "Cargando datos para region_dim.csv\n",
      "Datos ya existen para region_dim.csv\n",
      "Connection is established: Database tamales created\n",
      "Connection is established: Database teinvento created\n",
      "Data inserted into region_dim\n",
      "<class 'sqlite3.Error'>\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "Data inserted into tamales_inc table\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\OPI_repo\\procesado\\generador\\fuente\\202003\\tamales_inc_202003.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\OPI_repo\\procesado\\generador\\fuente\\202004\\tamales_inc_202004.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\OPI_repo\\procesado\\generador\\fuente\\202005\\tamales_inc_202005.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\OPI_repo\\procesado\\generador\\fuente\\202006\\tamales_inc_202006.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\OPI_repo\\procesado\\generador\\fuente\\202007\\tamales_inc_202007.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\OPI_repo\\procesado\\generador\\fuente\\202008\\tamales_inc_202008.csv\n",
      "Data inserted into region_dim\n",
      "Data inserted into product_dim_teinvento table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n",
      "Data inserted into teinvento_inc table\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    loadTamalesInc()\n",
    "    loadTeinventoInc()\n",
    "    con_tamales = generateDB.sql_connection('tamales_inc.db')\n",
    "    con_teinvento = generateDB.sql_connection('teinvento_inc.db')\n",
    "    \n",
    "    insertRowsTamales(con_tamales)\n",
    "    insertRowsTeinvento(con_teinvento)\n",
    "\n",
    "    con_tamales.close()\n",
    "    con_teinvento.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
