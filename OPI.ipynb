{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from watchdog.observers import Observer\n",
    "#from watchdog.events import FileSystemEventHandler\n",
    "\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "#import json\n",
    "#import time\n",
    "#import re\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "#import datetime\n",
    "#import time\n",
    "from itertools import product\n",
    "import generateDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDir = os.getcwd()\n",
    "tamales = os.path.join(rootDir,\"tamales_inc\")\n",
    "teinvento = os.path.join(rootDir,\"teinvento_inc\")\n",
    "rawDataPath = os.path.join(\"crudo\",\"generador\",\"fuente\")\n",
    "processedDataPath = os.path.join(\"procesado\",\"generador\",\"fuente\")\n",
    "dateDic = {\"Jan\":\"01\", \"Feb\":\"02\", \"Mar\":\"03\", \"Apr\":\"04\",\n",
    "           \"May\":\"05\", \"Jun\":\"06\", \"Jul\":\"07\", \"Aug\":\"08\",\n",
    "           \"Sep\":\"09\", \"Oct\":\"10\", \"Nov\":\"11\", \"Dec\":\"12\"}\n",
    "startDate = 0 #Esta variable puede ser útil para determinar fecha a partir de la cual quieres correr el código\n",
    "ventasCols = [\"year\", \"month\", \"country\", \"calorie_category\", \"flavor\", \"zone\", \"product_code\", \"product_name\", \"sales\"]\n",
    "factCols = [\"year\", \"month\", \"sales\", \"id_region\", \"id_product\"]\n",
    "prodCols = [\"id_product\", \"calorie_category\", \"product\", \"product_brand\", \"producer\"]\n",
    "regionCols = [\"id_region\", \"country\", \"region\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromDataPath(dataPath, cols):\n",
    "    dataFrames = []\n",
    "    for subdir, dirs, files in os.walk(dataPath):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                path2File = subdir+\"/\"+file\n",
    "                dataFrame = pd.read_csv(path2File, names = cols)\n",
    "                dataFrames.append(dataFrame)\n",
    "    df = pd.concat(dataFrames)    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMappingfile(tableName, realtimeFile = None, processedDataPath = processedDataPath):\n",
    "    \"\"\"Esta función devuelve region_table y product_table para los datos de tamales\"\"\"\n",
    "    processedLocation = os.path.join(rootDir,processedDataPath)\n",
    "    if not os.path.exists(processedLocation):\n",
    "        os.makedirs(processedLocation)\n",
    "    fileName = tableName+\"_tamales_inc.csv\"\n",
    "    MappingFile = os.path.join(processedLocation,fileName)\n",
    "    if os.path.isfile(MappingFile):\n",
    "        if realtimeFile:\n",
    "            mapping_df = pd.read_csv(path2TableFile)\n",
    "        else:\n",
    "            newtable_df = pd.read_csv(MappingFile)\n",
    "            print(\"This mapping file already exists for history data.\")\n",
    "            return(newtable_df)\n",
    "    else:\n",
    "        if tableName ==\"region_table\":\n",
    "            newCols = [\"country\", \"zone\"]\n",
    "            #We need a new index for region_id\n",
    "            keepIndex = True\n",
    "        elif tableName ==\"product_table\":\n",
    "            newCols = [\"product_code\",\"product_name\",\"flavor\",\"calorie_category\"]\n",
    "            keepIndex = False\n",
    "        else:\n",
    "            print(\"Only region_table and product_table are supported as mapping file.\")\n",
    "            return\n",
    "        dataFrames = []\n",
    "        for subdir, dirs, files in os.walk(tamales):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    path2File = subdir+\"/\"+file\n",
    "                    dataFrame = pd.read_csv(path2File, names = ventasCols)\n",
    "                    dataFrames.append(dataFrame)\n",
    "        df = pd.concat(dataFrames)\n",
    "        \n",
    "        newtable_df = df[newCols].drop_duplicates().reset_index(drop=True) \n",
    "        if keepIndex:\n",
    "            newtable_df.index.name = \"id_region\"\n",
    "        #product_code has a one to many mapping issue which will be overwritten\n",
    "        newtable_df.to_csv(MappingFile,index=keepIndex,sep=\",\")\n",
    "    return(newtable_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createData(newDataPath, df, clearData, datos, macro=False):\n",
    "    \"\"\"Toma DataFrame (df) y lo carga en el nuevo path (newDataPath) utilizando una etiqueta como macro. \n",
    "    Limpiará el cache si clearData es True\"\"\"\n",
    "    if macro:\n",
    "        filename = datos+\"_\"+macro+\".csv\"\n",
    "    else:\n",
    "        filename = datos+\".csv\"\n",
    "    newFile = os.path.join(newDataPath,filename)\n",
    "    #If file exists and we don't need to clear data\n",
    "    if clearData:\n",
    "            if not os.path.exists(newDataPath):\n",
    "                os.makedirs(newDataPath)\n",
    "            print(\"Limpiando y cargando datos para \"+filename)\n",
    "            df.to_csv(newFile,index=False,sep=\",\")\n",
    "    else:\n",
    "        if os.path.isfile(newFile) :\n",
    "            print(\"Datos ya existen para \"+filename)\n",
    "            return\n",
    "        else:\n",
    "            if not os.path.exists(newDataPath):\n",
    "                os.makedirs(newDataPath)\n",
    "            print(\"Cargando datos para \"+filename)\n",
    "            df.to_csv(newFile,index=False,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load2PathYYYYMM(df, destPath, yearCol = \"year\", monthCol = \"month\", datos=\"tamales_inc\", clearData = False, startDate = startDate):\n",
    "    \"\"\"\n",
    "    Carga datos que se encuentran en el mismo directorio y los acomoda en datos crudos de acuerdo a su fecha.\n",
    "    \"\"\"\n",
    "    #We get 6 digit pattern\n",
    "    dfGrouped = df[[yearCol,monthCol]].groupby([yearCol,monthCol]).size().reset_index().drop(columns=[0])\n",
    "    \n",
    "    for index, rows in dfGrouped.iterrows():\n",
    "        year = rows[yearCol]\n",
    "        month = rows[monthCol]\n",
    "        YYYYMM = str(year)+dateDic[month]\n",
    "        #Aquí podemos cargar datos desde la fecha en que especifiquemos\n",
    "        if startDate > int(YYYYMM):\n",
    "            continue\n",
    "        newDestPath = os.path.join(rootDir,destPath,YYYYMM)\n",
    "        #We filter records having year and month values\n",
    "        years = df[yearCol] == year\n",
    "        months = df[monthCol] == month\n",
    "        dataWithDate = df.loc[years & months]\n",
    "        \n",
    "        createData(newDestPath, dataWithDate, clearData, datos, YYYYMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTamalRaw2Proc(rawDataPath = rawDataPath, processedDataPath=processedDataPath, clearData = False, startDate = startDate):\n",
    "    \"\"\"\n",
    "    Loads processed data from Raw data. \n",
    "    \"\"\"\n",
    "    region_df = createMappingfile(\"region_table\")\n",
    "    product_df = createMappingfile(\"product_table\")\n",
    "    datos = \"tamales_inc\"\n",
    "    #We use regex to make sure we only load tamales\n",
    "    YYYYMMRegex = re.compile('({})_(\\d\\d\\d\\d\\d\\d)'.format(datos))\n",
    "    path = os.path.join(rootDir,rawDataPath)\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if not YYYYMMRegex.match(file):\n",
    "                continue\n",
    "            match = YYYYMMRegex.search(file)\n",
    "            YYYYMM = match.group(2) \n",
    "            #Aquí podemos cargar datos desde la fecha en que especifiquemos\n",
    "            if startDate > int(YYYYMM):\n",
    "                continue\n",
    "            newProcessedDataPath = os.path.join(rootDir,processedDataPath,YYYYMM)\n",
    "            #We keep relevant data\n",
    "            df1 = pd.read_csv(os.path.join(subdir,file))[[\"year\", \"month\", \"product_code\", \"zone\", \"sales\"]]\n",
    "            #We map region_id to zone\n",
    "            df1['zone'] = df1['zone'].map(region_df.set_index('zone')['id_region'])\n",
    "            df1Grouped = df1[[\"product_code\",\"zone\",\"sales\"]].groupby([\"product_code\",\"zone\"]).sum().reset_index()\n",
    "            df1Grouped.rename(columns = {\"product_code\":\"product\",\"sales\":\"monthly_sales\", \"zone\":\"id_region\"}, inplace = True)\n",
    "            \n",
    "            #Pendiente: arreglar error en 202003 cuando se incluyen nuevos valores\n",
    "            if 202002 < int(YYYYMM):\n",
    "                break\n",
    "            #Empezando el año\n",
    "            if file.endswith(\"01.csv\"):\n",
    "                df1Grouped[\"monthly_sales_acc\"] = df1Grouped[\"monthly_sales\"]\n",
    "                df1Grouped[\"diff_prev_month_perc\"] = None\n",
    "                df1Grouped_prev = df1Grouped.copy()\n",
    "            else:\n",
    "                #Aseguramos que el se creen columnas con datos para producto nuevo\n",
    "                df1Grouped[\"monthly_sales_acc\"] = df1Grouped[\"monthly_sales\"]\n",
    "                df1Grouped[\"diff_prev_month_perc\"] = None\n",
    "                for index, rows in df1Grouped_prev.iterrows():\n",
    "                        \n",
    "                    \n",
    "                    #Calculamos nuevo acumulado\n",
    "                    df1Grouped.loc[(df1Grouped[\"product\"]==rows[\"product\"]) & (df1Grouped[\"id_region\"]==rows[\"id_region\"]),\n",
    "                                   [\"monthly_sales_acc\"]] += rows[\"monthly_sales_acc\"]\n",
    "                    #Calculamos nuevo porcentaje\n",
    "                    prevSales = df1Grouped.loc[(df1Grouped[\"product\"]==rows[\"product\"]) & (df1Grouped[\"id_region\"]==rows[\"id_region\"]),\n",
    "                                               \"monthly_sales\"].item()\n",
    "                    newPercentage = ((prevSales/rows[\"monthly_sales\"] - 1)*100)\n",
    "                    df1Grouped.loc[(df1Grouped[\"product\"]==rows[\"product\"]) & (df1Grouped[\"id_region\"]==rows[\"id_region\"]),\n",
    "                               [\"diff_prev_month_perc\"]] = newPercentage\n",
    "\n",
    "                df1Grouped_prev = df1Grouped.copy()\n",
    "            \n",
    "            createData(newProcessedDataPath, df1Grouped, clearData, datos, YYYYMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def placeRaw2path(folder, destPath, rootDir=rootDir, clearData = False):\n",
    "    path2Folder = os.path.join(rootDir,folder)\n",
    "    newDataPath = os.path.join(rootDir,destPath)\n",
    "    for table in os.listdir(path2Folder):\n",
    "        subDir = os.path.join(path2Folder,table)\n",
    "        if table == \"fact_table\":\n",
    "            #Acomodaremos estos datos por fecha uniendo todas las particiones\n",
    "            dfData = loadFromDataPath(subDir,factCols)\n",
    "            load2PathYYYYMM(dfData,destPath,datos=folder)\n",
    "        elif table == \"product_dim\":\n",
    "            #Acomodaremos estos datos en el directorio fuente uniendo todas las particiones\n",
    "            dfData = loadFromDataPath(subDir,prodCols)\n",
    "            dfData = dfData[[\"id_product\", \"calorie_category\", \"product\", \"producer\"]]\n",
    "            createData(newDataPath, dfData, clearData, \"product_dim\")\n",
    "        else:\n",
    "            #Acomodaremos estos datos en el directorio fuente uniendo todas las particiones\n",
    "            dfData = loadFromDataPath(subDir,regionCols)\n",
    "            createData(newDataPath, dfData, clearData, \"region_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTamalesInc():    \n",
    "    print(\"Cargando datos de Tamales Inc.\")\n",
    "    df = loadFromDataPath(tamales, ventasCols)\n",
    "    print(\"Cargando datos crudos...\")\n",
    "    load2PathYYYYMM(df, rawDataPath)\n",
    "    print(\"Cargando datos procesados...\")\n",
    "    loadTamalRaw2Proc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTeinventoInc():\n",
    "    \"\"\"No proceso estos datos, sólo los ubico en los dataPath mencionados, y los separo si tienen columna year y month\"\"\"\n",
    "    print(\"Cargando datos de Teinvento Inc.\")\n",
    "    print(\"Cargando datos crudos...\")\n",
    "    placeRaw2path(\"teinvento_inc\", rawDataPath)\n",
    "    print(\"Cargando datos procesados...\")\n",
    "    placeRaw2path(\"teinvento_inc\", processedDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertRowsTamales(con_tamales):\n",
    "    generateDB.sqlInsertRegion(con_tamales, os.path.join(rootDir,processedDataPath,\"region_table_tamales_inc.csv\"))\n",
    "    generateDB.sqlInsertProductTamales(con_tamales, os.path.join(rootDir,processedDataPath,'product_table_tamales_inc.csv'))\n",
    "    datos = \"tamales_inc\"\n",
    "    YYYYMMRegex = re.compile('({})_(\\d\\d\\d\\d\\d\\d)'.format(datos))\n",
    "    path = os.path.join(rootDir,rawDataPath)\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if YYYYMMRegex.match(file):\n",
    "                match = YYYYMMRegex.search(file)\n",
    "                YYYYMM = match.group(2) \n",
    "                newFile = os.path.join(rootDir,processedDataPath,file)\n",
    "                generateDB.sqlInsertTamalesInc(con_tamales, newFile, YYYYMM)\n",
    "def insertRowsTeinvento(con_teinvento):\n",
    "    generateDB.sqlInsertRegion(con_teinvento, os.path.join(rootDir,processedDataPath,\"region_dim.csv\"))\n",
    "    generateDB.sqlInsertProductTeinvento(con_teinvento, os.path.join(rootDir,processedDataPath,'product_dim.csv'))\n",
    "    datos = \"teinvento_inc\"\n",
    "    YYYYMMRegex = re.compile('({})_(\\d\\d\\d\\d\\d\\d)'.format(datos))\n",
    "    path = os.path.join(rootDir,rawDataPath)\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if YYYYMMRegex.match(file):\n",
    "                match = YYYYMMRegex.search(file)\n",
    "                YYYYMM = match.group(2) \n",
    "                newFile = os.path.join(rootDir,processedDataPath,file)\n",
    "                generateDB.sqlInsertTeinventoInc(con_teinvento, newFile, YYYYMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de Tamales Inc.\n",
      "Cargando datos crudos...\n",
      "Datos ya existen para tamales_inc_201904.csv\n",
      "Datos ya existen para tamales_inc_201908.csv\n",
      "Datos ya existen para tamales_inc_201912.csv\n",
      "Datos ya existen para tamales_inc_201902.csv\n",
      "Datos ya existen para tamales_inc_201901.csv\n",
      "Datos ya existen para tamales_inc_201907.csv\n",
      "Datos ya existen para tamales_inc_201906.csv\n",
      "Datos ya existen para tamales_inc_201903.csv\n",
      "Datos ya existen para tamales_inc_201905.csv\n",
      "Datos ya existen para tamales_inc_201911.csv\n",
      "Datos ya existen para tamales_inc_201910.csv\n",
      "Datos ya existen para tamales_inc_201909.csv\n",
      "Datos ya existen para tamales_inc_202004.csv\n",
      "Datos ya existen para tamales_inc_202008.csv\n",
      "Datos ya existen para tamales_inc_202002.csv\n",
      "Datos ya existen para tamales_inc_202001.csv\n",
      "Datos ya existen para tamales_inc_202007.csv\n",
      "Datos ya existen para tamales_inc_202006.csv\n",
      "Datos ya existen para tamales_inc_202003.csv\n",
      "Datos ya existen para tamales_inc_202005.csv\n",
      "Cargando datos procesados...\n",
      "This mapping file already exists for history data.\n",
      "This mapping file already exists for history data.\n",
      "Datos ya existen para tamales_inc_201901.csv\n",
      "Datos ya existen para tamales_inc_201902.csv\n",
      "Datos ya existen para tamales_inc_201903.csv\n",
      "Datos ya existen para tamales_inc_201904.csv\n",
      "Datos ya existen para tamales_inc_201905.csv\n",
      "Datos ya existen para tamales_inc_201906.csv\n",
      "Datos ya existen para tamales_inc_201907.csv\n",
      "Datos ya existen para tamales_inc_201908.csv\n",
      "Datos ya existen para tamales_inc_201909.csv\n",
      "Datos ya existen para tamales_inc_201910.csv\n",
      "Datos ya existen para tamales_inc_201911.csv\n",
      "Datos ya existen para tamales_inc_201912.csv\n",
      "Datos ya existen para tamales_inc_202001.csv\n",
      "Datos ya existen para tamales_inc_202002.csv\n",
      "Cargando datos de Teinvento Inc.\n",
      "Cargando datos crudos...\n",
      "Datos ya existen para teinvento_inc_201804.csv\n",
      "Datos ya existen para teinvento_inc_201808.csv\n",
      "Datos ya existen para teinvento_inc_201812.csv\n",
      "Datos ya existen para teinvento_inc_201802.csv\n",
      "Datos ya existen para teinvento_inc_201801.csv\n",
      "Datos ya existen para teinvento_inc_201807.csv\n",
      "Datos ya existen para teinvento_inc_201806.csv\n",
      "Datos ya existen para teinvento_inc_201803.csv\n",
      "Datos ya existen para teinvento_inc_201805.csv\n",
      "Datos ya existen para teinvento_inc_201811.csv\n",
      "Datos ya existen para teinvento_inc_201810.csv\n",
      "Datos ya existen para teinvento_inc_201809.csv\n",
      "Datos ya existen para teinvento_inc_201904.csv\n",
      "Datos ya existen para teinvento_inc_201908.csv\n",
      "Datos ya existen para teinvento_inc_201912.csv\n",
      "Datos ya existen para teinvento_inc_201902.csv\n",
      "Datos ya existen para teinvento_inc_201901.csv\n",
      "Datos ya existen para teinvento_inc_201907.csv\n",
      "Datos ya existen para teinvento_inc_201906.csv\n",
      "Datos ya existen para teinvento_inc_201903.csv\n",
      "Datos ya existen para teinvento_inc_201905.csv\n",
      "Datos ya existen para teinvento_inc_201911.csv\n",
      "Datos ya existen para teinvento_inc_201910.csv\n",
      "Datos ya existen para teinvento_inc_201909.csv\n",
      "Datos ya existen para teinvento_inc_202004.csv\n",
      "Datos ya existen para teinvento_inc_202002.csv\n",
      "Datos ya existen para teinvento_inc_202001.csv\n",
      "Datos ya existen para teinvento_inc_202003.csv\n",
      "Datos ya existen para teinvento_inc_202005.csv\n",
      "Datos ya existen para product_dim.csv\n",
      "Datos ya existen para region_dim.csv\n",
      "Datos ya existen para region_dim.csv\n",
      "Cargando datos procesados...\n",
      "Datos ya existen para teinvento_inc_201804.csv\n",
      "Datos ya existen para teinvento_inc_201808.csv\n",
      "Datos ya existen para teinvento_inc_201812.csv\n",
      "Datos ya existen para teinvento_inc_201802.csv\n",
      "Datos ya existen para teinvento_inc_201801.csv\n",
      "Datos ya existen para teinvento_inc_201807.csv\n",
      "Datos ya existen para teinvento_inc_201806.csv\n",
      "Datos ya existen para teinvento_inc_201803.csv\n",
      "Datos ya existen para teinvento_inc_201805.csv\n",
      "Datos ya existen para teinvento_inc_201811.csv\n",
      "Datos ya existen para teinvento_inc_201810.csv\n",
      "Datos ya existen para teinvento_inc_201809.csv\n",
      "Datos ya existen para teinvento_inc_201904.csv\n",
      "Datos ya existen para teinvento_inc_201908.csv\n",
      "Datos ya existen para teinvento_inc_201912.csv\n",
      "Datos ya existen para teinvento_inc_201902.csv\n",
      "Datos ya existen para teinvento_inc_201901.csv\n",
      "Datos ya existen para teinvento_inc_201907.csv\n",
      "Datos ya existen para teinvento_inc_201906.csv\n",
      "Datos ya existen para teinvento_inc_201903.csv\n",
      "Datos ya existen para teinvento_inc_201905.csv\n",
      "Datos ya existen para teinvento_inc_201911.csv\n",
      "Datos ya existen para teinvento_inc_201910.csv\n",
      "Datos ya existen para teinvento_inc_201909.csv\n",
      "Datos ya existen para teinvento_inc_202004.csv\n",
      "Datos ya existen para teinvento_inc_202002.csv\n",
      "Datos ya existen para teinvento_inc_202001.csv\n",
      "Datos ya existen para teinvento_inc_202003.csv\n",
      "Datos ya existen para teinvento_inc_202005.csv\n",
      "Cargando datos para product_dim.csv\n",
      "Datos ya existen para region_dim.csv\n",
      "Datos ya existen para region_dim.csv\n",
      "Connection is established: Database tamales created\n",
      "Connection is established: Database teinvento created\n",
      "<class 'sqlite3.Error'>\n",
      "<class 'sqlite3.Error'>\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_201901.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_201902.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_201903.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_201904.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_201905.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_201906.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_201907.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_201908.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_201909.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_201910.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_201911.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_201912.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_202001.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_202002.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_202003.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_202004.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_202005.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_202006.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_202007.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\tamales_inc_202008.csv\n",
      "<class 'sqlite3.Error'>\n",
      "Data inserted into product_dim_teinvento table\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201801.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201802.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201803.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201804.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201805.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201806.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201807.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201808.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201809.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201810.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201811.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201812.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201901.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201902.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201903.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201904.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201905.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201906.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201907.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201908.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201909.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201910.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201911.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_201912.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_202001.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_202002.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_202003.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_202004.csv\n",
      "ERROR trying to insert, could not find file C:\\Users\\edson\\OneDrive\\Documentos\\BigData\\Aldo\\procesado\\generador\\fuente\\teinvento_inc_202005.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    loadTamalesInc()\n",
    "    loadTeinventoInc()\n",
    "    con_tamales = generateDB.sql_connection('tamales_inc.db')\n",
    "    con_teinvento = generateDB.sql_connection('teinvento_inc.db')\n",
    "    \n",
    "    insertRowsTamales(con_tamales)\n",
    "    insertRowsTeinvento(con_teinvento)\n",
    "\n",
    "    con_tamales.close()\n",
    "    con_teinvento.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_df = pd.read_csv(os.path.join(rootDir,processedDataPath,\"region_table_tamales_inc.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_region</th>\n",
       "      <th>country</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Centro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>E. Privados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Norte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Sur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_region country         zone\n",
       "0          0  Mexico       Centro\n",
       "1          1  Mexico  E. Privados\n",
       "2          2  Mexico        Norte\n",
       "3          3  Mexico          Sur"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(os.path.join(rootDir,rawDataPath,\"201901\",\"tamales_inc_201901.csv\"))[[\"year\", \"month\", \"product_code\", \"zone\", \"sales\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_code</th>\n",
       "      <th>zone</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206050084</td>\n",
       "      <td>E. Privados</td>\n",
       "      <td>7.076249e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206050084</td>\n",
       "      <td>Norte</td>\n",
       "      <td>1.180721e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206050084</td>\n",
       "      <td>Sur</td>\n",
       "      <td>1.960398e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>206051447</td>\n",
       "      <td>E. Privados</td>\n",
       "      <td>7.000671e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>206051447</td>\n",
       "      <td>Norte</td>\n",
       "      <td>4.978629e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>206051447</td>\n",
       "      <td>Sur</td>\n",
       "      <td>2.606738e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>206054370</td>\n",
       "      <td>Centro</td>\n",
       "      <td>6.597472e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>206054370</td>\n",
       "      <td>E. Privados</td>\n",
       "      <td>8.590493e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>206054370</td>\n",
       "      <td>Norte</td>\n",
       "      <td>1.702286e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>206054370</td>\n",
       "      <td>Sur</td>\n",
       "      <td>1.030975e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ABYT055818</td>\n",
       "      <td>Centro</td>\n",
       "      <td>4.835082e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ABYT055818</td>\n",
       "      <td>E. Privados</td>\n",
       "      <td>1.358863e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ABYT055818</td>\n",
       "      <td>Norte</td>\n",
       "      <td>1.231910e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ABYT055818</td>\n",
       "      <td>Sur</td>\n",
       "      <td>5.105248e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>Centro</td>\n",
       "      <td>1.899252e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>E. Privados</td>\n",
       "      <td>1.702395e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>Norte</td>\n",
       "      <td>2.937847e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>Sur</td>\n",
       "      <td>6.343492e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_code         zone         sales\n",
       "0     206050084  E. Privados  7.076249e+05\n",
       "1     206050084        Norte  1.180721e+06\n",
       "2     206050084          Sur  1.960398e+06\n",
       "3     206051447  E. Privados  7.000671e+03\n",
       "4     206051447        Norte  4.978629e+04\n",
       "5     206051447          Sur  2.606738e+05\n",
       "6     206054370       Centro  6.597472e+03\n",
       "7     206054370  E. Privados  8.590493e+02\n",
       "8     206054370        Norte  1.702286e+02\n",
       "9     206054370          Sur  1.030975e+02\n",
       "10   ABYT055818       Centro  4.835082e+03\n",
       "11   ABYT055818  E. Privados  1.358863e+02\n",
       "12   ABYT055818        Norte  1.231910e+03\n",
       "13   ABYT055818          Sur  5.105248e+03\n",
       "14   ABYT057271       Centro  1.899252e+06\n",
       "15   ABYT057271  E. Privados  1.702395e+05\n",
       "16   ABYT057271        Norte  2.937847e+05\n",
       "17   ABYT057271          Sur  6.343492e+05"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1Group = df1[[\"product_code\",\"zone\",\"sales\"]].groupby([\"product_code\",\"zone\"]).sum()\n",
    "df1Group.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>product_code</th>\n",
       "      <th>zone</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>Jan</td>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>0</td>\n",
       "      <td>2.793307e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>Jan</td>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>0</td>\n",
       "      <td>9.678073e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>Jan</td>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.080000e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>Jan</td>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.420000e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>Jan</td>\n",
       "      <td>206054370</td>\n",
       "      <td>0</td>\n",
       "      <td>4.970000e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2019</td>\n",
       "      <td>Jan</td>\n",
       "      <td>206050084</td>\n",
       "      <td>3</td>\n",
       "      <td>6.756187e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2019</td>\n",
       "      <td>Jan</td>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.620000e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2019</td>\n",
       "      <td>Jan</td>\n",
       "      <td>206051447</td>\n",
       "      <td>3</td>\n",
       "      <td>8.103046e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2019</td>\n",
       "      <td>Jan</td>\n",
       "      <td>206050084</td>\n",
       "      <td>3</td>\n",
       "      <td>5.538376e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2019</td>\n",
       "      <td>Jan</td>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>3</td>\n",
       "      <td>4.909296e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year month product_code  zone         sales\n",
       "0    2019   Jan   ABYT057271     0  2.793307e+04\n",
       "1    2019   Jan   ABYT057271     0  9.678073e+05\n",
       "2    2019   Jan   ABYT057271     0 -2.080000e-13\n",
       "3    2019   Jan   ABYT057271     0 -9.420000e-11\n",
       "4    2019   Jan    206054370     0  4.970000e-14\n",
       "..    ...   ...          ...   ...           ...\n",
       "495  2019   Jan    206050084     3  6.756187e+04\n",
       "496  2019   Jan   ABYT057271     3 -4.620000e-14\n",
       "497  2019   Jan    206051447     3  8.103046e+03\n",
       "498  2019   Jan    206050084     3  5.538376e+04\n",
       "499  2019   Jan   ABYT057271     3  4.909296e+03\n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['zone'] = df1['zone'].map(reg_df.set_index('zone')['id_region'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_code</th>\n",
       "      <th>zone</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206050084</td>\n",
       "      <td>1</td>\n",
       "      <td>7.076249e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206050084</td>\n",
       "      <td>2</td>\n",
       "      <td>1.180721e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206050084</td>\n",
       "      <td>3</td>\n",
       "      <td>1.960398e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>206051447</td>\n",
       "      <td>1</td>\n",
       "      <td>7.000671e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>206051447</td>\n",
       "      <td>2</td>\n",
       "      <td>4.978629e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>206051447</td>\n",
       "      <td>3</td>\n",
       "      <td>2.606738e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>206054370</td>\n",
       "      <td>0</td>\n",
       "      <td>6.597472e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>206054370</td>\n",
       "      <td>1</td>\n",
       "      <td>8.590493e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>206054370</td>\n",
       "      <td>2</td>\n",
       "      <td>1.702286e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>206054370</td>\n",
       "      <td>3</td>\n",
       "      <td>1.030975e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ABYT055818</td>\n",
       "      <td>0</td>\n",
       "      <td>4.835082e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ABYT055818</td>\n",
       "      <td>1</td>\n",
       "      <td>1.358863e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ABYT055818</td>\n",
       "      <td>2</td>\n",
       "      <td>1.231910e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ABYT055818</td>\n",
       "      <td>3</td>\n",
       "      <td>5.105248e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>0</td>\n",
       "      <td>1.899252e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>1</td>\n",
       "      <td>1.702395e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>2</td>\n",
       "      <td>2.937847e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ABYT057271</td>\n",
       "      <td>3</td>\n",
       "      <td>6.343492e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_code  zone         sales\n",
       "0     206050084     1  7.076249e+05\n",
       "1     206050084     2  1.180721e+06\n",
       "2     206050084     3  1.960398e+06\n",
       "3     206051447     1  7.000671e+03\n",
       "4     206051447     2  4.978629e+04\n",
       "5     206051447     3  2.606738e+05\n",
       "6     206054370     0  6.597472e+03\n",
       "7     206054370     1  8.590493e+02\n",
       "8     206054370     2  1.702286e+02\n",
       "9     206054370     3  1.030975e+02\n",
       "10   ABYT055818     0  4.835082e+03\n",
       "11   ABYT055818     1  1.358863e+02\n",
       "12   ABYT055818     2  1.231910e+03\n",
       "13   ABYT055818     3  5.105248e+03\n",
       "14   ABYT057271     0  1.899252e+06\n",
       "15   ABYT057271     1  1.702395e+05\n",
       "16   ABYT057271     2  2.937847e+05\n",
       "17   ABYT057271     3  6.343492e+05"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1Grouped = df1[[\"product_code\",\"zone\",\"sales\"]].groupby([\"product_code\",\"zone\"]).sum().reset_index()\n",
    "df1Grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('206050084', 1)\n",
    "('206050084', 2)\n",
    "('206050084', 3)\n",
    "('206050084', 0)\n",
    "('206051447', 1)\n",
    "('206051447', 2)\n",
    "('206051447', 3)\n",
    "('206051447', 0)\n",
    "('206054370', 1)\n",
    "('206054370', 2)\n",
    "('206054370', 3)\n",
    "('206054370', 0)\n",
    "('ABYT055818', 1)\n",
    "('ABYT055818', 2)\n",
    "('ABYT055818', 3)\n",
    "('ABYT055818', 0)\n",
    "('ABYT057271', 1)\n",
    "('ABYT057271', 2)\n",
    "('ABYT057271', 3)\n",
    "('ABYT057271', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      206050084\n",
       "1      206050084\n",
       "2      206050084\n",
       "3      206051447\n",
       "4      206051447\n",
       "5      206051447\n",
       "6      206054370\n",
       "7      206054370\n",
       "8      206054370\n",
       "9      206054370\n",
       "10    ABYT055818\n",
       "11    ABYT055818\n",
       "12    ABYT055818\n",
       "13    ABYT055818\n",
       "14    ABYT057271\n",
       "15    ABYT057271\n",
       "16    ABYT057271\n",
       "17    ABYT057271\n",
       "Name: product_code, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1Grouped[\"product_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('206050084', 1),\n",
       " ('206050084', 2),\n",
       " ('206050084', 3),\n",
       " ('206050084', 0),\n",
       " ('206051447', 1),\n",
       " ('206051447', 2),\n",
       " ('206051447', 3),\n",
       " ('206051447', 0),\n",
       " ('206054370', 1),\n",
       " ('206054370', 2),\n",
       " ('206054370', 3),\n",
       " ('206054370', 0),\n",
       " ('ABYT055818', 1),\n",
       " ('ABYT055818', 2),\n",
       " ('ABYT055818', 3),\n",
       " ('ABYT055818', 0),\n",
       " ('ABYT057271', 1),\n",
       " ('ABYT057271', 2),\n",
       " ('ABYT057271', 3),\n",
       " ('ABYT057271', 0)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "uniqueGrouped = list(product(df1Grouped[\"product_code\"].unique(),df1Grouped[\"zone\"].unique()))\n",
    "uniqueGrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n",
      "('206050084', 1)\n",
      "hola\n",
      "('206050084', 2)\n",
      "hola\n",
      "('206050084', 3)\n",
      "hola\n",
      "('206051447', 1)\n",
      "hola\n",
      "('206051447', 2)\n",
      "hola\n",
      "('206051447', 3)\n",
      "hola\n",
      "('206054370', 0)\n",
      "hola\n",
      "('206054370', 1)\n",
      "hola\n",
      "('206054370', 2)\n",
      "hola\n",
      "('206054370', 3)\n",
      "hola\n",
      "('ABYT055818', 0)\n",
      "hola\n",
      "('ABYT055818', 1)\n",
      "hola\n",
      "('ABYT055818', 2)\n",
      "hola\n",
      "('ABYT055818', 3)\n",
      "hola\n",
      "('ABYT057271', 0)\n",
      "hola\n",
      "('ABYT057271', 1)\n",
      "hola\n",
      "('ABYT057271', 2)\n",
      "hola\n",
      "('ABYT057271', 3)\n"
     ]
    }
   ],
   "source": [
    "for index, rows in df1Grouped.iterrows():\n",
    "    df1Grouped.loc[df1Grouped[\"product_code\",]==rows[\"product_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('206050084', 0), ('206051447', 0)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueGrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1Grouped.loc[df1Grouped[\"product\"]==rows[\"product\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
